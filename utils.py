import numpy as np
from random import randint, choice
import itertools
from typing import Tuple, Iterable, List, Callable
import operator
import functools
import math
from torch import nn
import torch
import copy

class NotEnoughGPURam(Exception):
    pass

def i_arr_mul(array: Iterable) -> int:
    return functools.reduce(operator.mul, array, 1)

def KiB(coef: float = 1) -> int:
    return math.floor(coef * 1024)

def MiB(coef: float = 1) -> int:
    return math.floor(1024 * KiB(coef))

def GiB(coef: float = 1) -> int:
    return math.floor(1024 * MiB(coef))

def round_to(number: int, modulo: int) -> int:
    return (number // modulo) * modulo

class InspectNetwork:
    """
        Torch network Inspector for memory footprint
    """
    def __init__(self, network: nn.Module, input_shape: Tuple):
        """
            @param network: the neural network to be inspected
            @param input_shape: input shape of the network.
            Network will be copied and analysed on cpu with one forward.
        """
        self.__parameter_count = sum(param.nelement() for param in network.parameters())
        self.__network = network
        self._variable_count = 0
        self.__input_shape = input_shape
        self.__inspect_output_shapes()

    def __inspect_output_shapes(self):
        copied_network = copy.deepcopy(self.__network).cpu()

        def hook(_cls_instance, _inp, _out):
            print('-' * 25, _cls_instance.__class__.__name__, '-' * 25)
            print(f'input: {_inp[0].data.size()}, output: {_out.data.size()}')
            print(f'parameters: {[x.shape for x in _cls_instance.parameters()]}')
            print()
            output_variable_count = i_arr_mul(_out.data.size())
            grad_variable_count = 0
            if _out.requires_grad:
                grad_variable_count = output_variable_count
            self._variable_count += grad_variable_count + output_variable_count

        for layer in copied_network.modules():
            layer.register_forward_hook(hook)
        sample = torch.zeros(self.__input_shape).float()
        copied_network.forward(sample)
        del copied_network

    def get_parameter_count(self):
        return self.__parameter_count

    def get_intermediate_variable_count(self):
        return self._variable_count
        

class Config:
    def __init__(self, 
            cpu_memory_limit: int, # in bytes
            gpu_memory_limit: int, # in bytes
            total_sample_count: int,
            input_shape: Tuple,
            output_shape: Tuple,
            network: nn.Module,
            input_dtype: type = np.float64,
            output_dtype: type = np.float64,
            one_read_count: int = 1,
            preferred_batch_size: int = None
            ):
        """
            @param cpu_memory_limit: Limit of the memory that a  sample batch is allowed to use in CPU RAM.
            @param gpu_memory_limit: Limit of the memory that a input&output batch is allowed to use in GPU RAM.
            @param total_sample_count: total sample count in the memory.
            @param parameter_count: count of the parameters in the model.
            @param input_shape: Input shape of a sample. 
            @param output_shape: Output_shape of a sample.
            @param input_dtype: data type of the input samples.
            @param output_dtype: data type of the output samples.
            @param one_read_count: Count of sample generated by read_input.
            @param preferred_batch_size: if it is applicable, preferred_batch_size - preferred_batch_size % one_read_count will be used as batch size
        """
        self.cpu_memory_limit = cpu_memory_limit # bytes.
        self.gpu_memory_limit = gpu_memory_limit # bytes.
        self._total_sample_count = total_sample_count
        self.input_shape = input_shape
        self.output_shape = output_shape
        self.input_dtype = input_dtype
        self.output_dtype = output_dtype
        self.one_read_count = one_read_count
        try:
            self.input_item_size = self.input_dtype().itemsize
            self.output_item_size = self.output_dtype().itemsize
        except Exception as e:
            print(f'INVALID DTYPE GIVEN!. {e!r}')
            print('elem size is set to default (8 bytes)')
            self.input_item_size = 8
            self.output_item_size = 8
        network_inspector = InspectNetwork(network=network, input_shape=input_shape)
        self.parameter_count = network_inspector.get_parameter_count()
        self.total_intermediate_variable_count = network_inspector.get_intermediate_variable_count()
        self.preferred_batch_size = preferred_batch_size or (self.gpu_memory_limit - self.model_memory_usage) // self.single_batch_memory_usage
        if self.model_memory_usage >= self.gpu_memory_limit:
            raise NotEnoughGPURam('Parameters exhausts GPU Ram. Please either increase GPU limit, or shrink network.')
        if self.batch_size <= 0:
            raise NotEnoughGPURam('Not Enough Space in GPU for a single batch!')

    @property
    def input_size(self):
        return self.input_item_size * i_arr_mul(self.input_shape)

    @property
    def output_size(self):
        return self.output_item_size * i_arr_mul(self.output_shape)

    @property
    def sample_size(self):
        return self.input_size + self.output_size

    @property
    def single_batch_memory_usage(self):
        return self.sample_size + self.total_intermediate_variable_count * self.input_item_size
    
    @property
    def model_memory_usage(self):
        return self.parameter_count * self.input_item_size

    @property
    def batch_size(self):
        return round_to(min((self.gpu_memory_limit - self.model_memory_usage) // self.single_batch_memory_usage, self.preferred_batch_size), self.one_read_count)

    @property
    def sample_batch_memory_footprint(self):
        return self.batch_size * self.sample_size

    @property
    def total_sample_count(self):
        return round_to(self._total_sample_count, self.batch_size)

    @property
    def _allowed_sample_count(self):
        return min(self.total_sample_count, self.cpu_memory_limit // self.sample_size)

    @property
    def allowed_sample_count(self):
        return round_to(self._allowed_sample_count, self.batch_size)

    @property
    def batch_count(self):
        return self.allowed_sample_count // self.batch_size

 
class Feeder:
    """
        This is the sample file feeder for the deep learning model.
        It reads samples batch by batch and generates a generator that 
        returns a batch of sample that to be used as an epoch in training 
        process.
    """
    path = '/home/kursat/Desktop/Projects/Menrva/Dataset/nsynth-valid.jsonwav/nsynth-valid'

    def __init__(
            self,
            configs: Config, 
            input_list: List[str], 
            output_list: List[str],
            read_input: Callable[[str], Iterable[np.ndarray]],
            get_output: Callable[[str], Iterable[np.ndarray]],
            apply_on_input_batch: Callable[[Iterable[np.ndarray]], np.ndarray] = None,
            apply_on_output_batch: Callable[[Iterable[np.ndarray]], np.ndarray] = None,
            ) -> Iterable[np.ndarray]:
        """
            Reads files from hard drive and returns,
            by not violating the memory restrictions
            batch by batch.
            i.e. if memory limit is 1gb, 
            calculates memory_footprint = 1gb - sample_batch_memory_footprint(batch_size)
            Returns a generator that reads 'memory_footprint bytes' of music in each pass.
            
            @param config: Config instance for resource limitations and requirements,
            @param input_list: List of input paths that to be read from disk,
            @param output_list: List of output paths that to be read from disk,
            @param read_input: A Callable taking one parameter of input path. Should return an iterator containin np.ndarray items,
            @param get_output: A Callable taking one parameter of output path. Should return an iterator containin np.ndarray items,
            @param apply_on_input_batch: A Callable taking one parameter of input batch. Should return a np.ndarray,
            @param apply_on_output_batch: A Callable taking one parameter of output batch. Should return a np.ndarray,
        """
        if apply_on_input_batch is None:
            apply_on_input_batch = lambda x: np.array(list(x), dtype=configs.input_dtype)
        if apply_on_output_batch is None:
            apply_on_output_batch = lambda x: np.array(list(x), dtype=configs.output_dtype)
        self.apply_on_input_batch = apply_on_input_batch
        self.apply_on_output_batch = apply_on_output_batch
        self.read_input = read_input
        self.get_output = get_output
        self.input_list = input_list
        self.output_list = output_list
        self.configs = configs
        self.read_batch_count = configs.one_read_count * configs.total_sample_count // configs.allowed_sample_count 
        self.input_output_list_length = configs.allowed_sample_count // configs.one_read_count
 
    def _get_samples_one_pass(self) -> Iterable[np.ndarray]:
        start_from = 0
        read_until = self.input_output_list_length
        for i in range(self.read_batch_count):
            samples_on_memory = (
                    inp for inp_path in self.input_list[
                        start_from: read_until
                        ]
                    for inp in self.read_input(inp_path)
                    )
            labels_on_memory = (
                    out for out_path in self.output_list[
                        start_from: read_until
                        ]
                    for out in self.get_output(out_path)
                    )
            for batch in range(self.configs.batch_count):
                yield self.apply_on_input_batch(
                        (sample for sample in itertools.islice(samples_on_memory, self.configs.batch_size))
                        ), self.apply_on_output_batch(
                        (label for label in itertools.islice(labels_on_memory, self.configs.batch_size))
                        )
            start_from += self.input_output_list_length
            read_until += self.input_output_list_length

    def generate_epochs(self, epoch_count:int = 1) -> Iterable[Iterable[np.ndarray]]:
        samples_generator = self._get_samples_one_pass()
        if epoch_count == 1:
            yield samples_generator
        else:
            yield from itertools.tee(samples_generator, epoch_count)
